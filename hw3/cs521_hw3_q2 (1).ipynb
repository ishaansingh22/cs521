{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.15.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa9s5MdS11lh",
        "outputId": "faf9db92-e504-4df0-bb4e-b9b258a0b79b"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext==0.15.2 in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (4.66.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (2.32.3)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (2.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (1.26.4)\n",
            "Requirement already satisfied: torchdata==0.6.1 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (0.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext==0.15.2) (2.0.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.1->torchtext==0.15.2) (2.2.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchtext==0.15.2) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchtext==0.15.2) (0.44.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchtext==0.15.2) (3.30.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchtext==0.15.2) (18.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.2) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.2) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.2) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchtext==0.15.2) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchtext==0.15.2) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5PTRchS26Ak",
        "outputId": "eee27763-4339-4f27-b663-d97e844190f2"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"torchtext version: {torchtext.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3LRSd582Ojt",
        "outputId": "ee5065f5-c679-4f4e-ee69-6a28235079dd"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.0.1+cu117\n",
            "torchtext version: 0.15.2+cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.data import get_tokenizer\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "vec = torchtext.vocab.GloVe(name='6B', dim=100)\n",
        "\n",
        "ds = load_dataset(\"stanfordnlp/imdb\")\n",
        "\n",
        "def process_dataset(split):\n",
        "    bows = []\n",
        "    gt = []\n",
        "    for example in split:\n",
        "        tokens = tokenizer(example[\"text\"])\n",
        "        tokens = [token for token in tokens if token in vec.stoi]\n",
        "        if tokens:\n",
        "            embeddings = vec.get_vecs_by_tokens(tokens, lower_case_backup=True)\n",
        "            bow = embeddings.mean(dim=0)\n",
        "        else:\n",
        "            bow = torch.zeros(vec.dim)\n",
        "        bows.append(bow)\n",
        "        gt.append(example[\"label\"])\n",
        "    data = torch.stack(bows)\n",
        "    labels = torch.tensor(gt)\n",
        "    return torch.utils.data.TensorDataset(data, labels)\n",
        "\n",
        "train_dataset = process_dataset(ds[\"train\"])\n",
        "test_dataset = process_dataset(ds[\"test\"])\n",
        "\n",
        "validation_split = 0.1\n",
        "train_size = int((1 - validation_split) * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Training samples: {len(train_subset)}\")\n",
        "print(f\"Validation samples: {len(val_subset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcQqwpWXac98",
        "outputId": "6b450ff4-c5b1-4c70-e0fe-faa8136c9655"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 22500\n",
            "Validation samples: 2500\n",
            "Test samples: 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class TextIBPModel(nn.Module):\n",
        "    def __init__(self, embedding_dim=100, hidden_dim=100, num_classes=2):\n",
        "        super(TextIBPModel, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.linear = nn.Linear(embedding_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: Tensor of shape (batch_size, embedding_dim)\n",
        "        \"\"\"\n",
        "        x = self.linear(x)\n",
        "        x = self.relu(x)\n",
        "        # x is now (batch_size, hidden_dim)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        logits = self.fc2(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "5zdy5aQMZ36E"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IntervalPropagation:\n",
        "    def __init__(self, lower, upper):\n",
        "        self.lower = lower\n",
        "        self.upper = upper\n",
        "\n",
        "    def __add__(self, other):\n",
        "        return IntervalPropagation(self.lower + other, self.upper + other)\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        if isinstance(other, IntervalPropagation):\n",
        "            lower = torch.minimum(torch.minimum(self.lower * other.lower, self.lower * other.upper),\n",
        "                                  torch.minimum(self.upper * other.lower, self.upper * other.upper))\n",
        "            upper = torch.maximum(torch.maximum(self.lower * other.lower, self.lower * other.upper),\n",
        "                                  torch.maximum(self.upper * other.lower, self.upper * other.upper))\n",
        "            return IntervalPropagation(lower, upper)\n",
        "        else:\n",
        "            lower = torch.minimum(self.lower * other, self.upper * other)\n",
        "            upper = torch.maximum(self.lower * other, self.upper * other)\n",
        "            return IntervalPropagation(lower, upper)"
      ],
      "metadata": {
        "id": "Wvn91512Z44o"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def propagate_layer(layer, interval):\n",
        "    \"\"\"\n",
        "    layer: nn.Linear layer\n",
        "    interval: IntervalPropagation object\n",
        "    \"\"\"\n",
        "    lower = F.linear(interval.lower, layer.weight, layer.bias)\n",
        "    upper = F.linear(interval.upper, layer.weight, layer.bias)\n",
        "    return IntervalPropagation(lower, upper)\n",
        "\n",
        "def relu_bounds(lower, upper):\n",
        "    \"\"\"Propagate bounds through a ReLU layer.\"\"\"\n",
        "    return F.relu(lower), F.relu(upper)\n",
        "\n",
        "def propagate_model(model, interval):\n",
        "    \"\"\"\n",
        "    model: TextIBPModel\n",
        "    interval: IntervalPropagation object\n",
        "    Returns: (lower_logits, upper_logits)\n",
        "    \"\"\"\n",
        "    # Layer 1: Linear -> ReLU\n",
        "    interval = propagate_layer(model.linear, interval)\n",
        "    lower, upper = relu_bounds(interval.lower, interval.upper)\n",
        "    interval = IntervalPropagation(lower, upper)\n",
        "\n",
        "    # Layer 2: Feedforward Layer 1 -> ReLU\n",
        "    interval = propagate_layer(model.fc1, interval)\n",
        "    lower, upper = relu_bounds(interval.lower, interval.upper)\n",
        "    interval = IntervalPropagation(lower, upper)\n",
        "\n",
        "    # Layer 3: Feedforward Layer 2 (Output Layer)\n",
        "    interval = propagate_layer(model.fc2, interval)\n",
        "\n",
        "    return interval.lower, interval.upper"
      ],
      "metadata": {
        "id": "vtP3RFo3Z7ot"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ibp_loss(logits, true_labels, logits_robust, kappa):\n",
        "    nominal_loss = F.cross_entropy(logits, true_labels)\n",
        "    robust_loss = F.cross_entropy(logits_robust, true_labels)\n",
        "    return kappa * nominal_loss + (1 - kappa) * robust_loss\n"
      ],
      "metadata": {
        "id": "21mZGTPqaJwV"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def schedule_kappa(step, total_steps):\n",
        "    initial_kappa = 1.0\n",
        "    final_kappa = 0.5\n",
        "    return max(final_kappa, initial_kappa - (initial_kappa - final_kappa) * (step / total_steps))\n",
        "\n",
        "def schedule_epsilon(step, total_steps, target_epsilon=0.1):\n",
        "    return min(target_epsilon, target_epsilon * (step / total_steps))"
      ],
      "metadata": {
        "id": "ApNQZbRWaAGr"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ibp_forward(model, x, eps):\n",
        "    \"\"\"\n",
        "    model: TextIBPModel\n",
        "    x: Input tensor of shape (batch_size, embedding_dim)\n",
        "    eps: Perturbation size\n",
        "    Returns: (lower_logits, upper_logits)\n",
        "    \"\"\"\n",
        "    lower_bound = x - eps\n",
        "    upper_bound = x + eps\n",
        "    interval = IntervalPropagation(lower_bound, upper_bound)\n",
        "    lower_logits, upper_logits = propagate_model(model, interval)\n",
        "    return lower_logits, upper_logits"
      ],
      "metadata": {
        "id": "m_5hp1GxaCjX"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def train_ibp_with_early_stopping(model, optimizer, train_loader, val_loader, total_steps, eps_target=0.1, device='cpu', patience=500):\n",
        "    model.train()\n",
        "    current_step = 0\n",
        "    best_verified_acc = 0\n",
        "    patience_counter = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    while current_step < total_steps:\n",
        "        for batch_data, batch_labels in train_loader:\n",
        "            if current_step >= total_steps:\n",
        "                break\n",
        "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "            logits = model(batch_data)\n",
        "            lower_logits, upper_logits = ibp_forward(model, batch_data, eps_target)\n",
        "            logits_robust = (lower_logits + upper_logits) / 2\n",
        "            kappa = schedule_kappa(current_step, total_steps)\n",
        "            epsilon_train = schedule_epsilon(current_step, total_steps, eps_target)\n",
        "            loss = ibp_loss(logits, batch_labels, logits_robust, kappa)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if current_step % 100 == 0:\n",
        "                verified_acc = compute_verified_accuracy(model, val_loader, eps_target, device)\n",
        "                standard_acc = compute_standard_accuracy(model, val_loader, device)\n",
        "                print(f\"Step [{current_step}/{total_steps}], Loss: {loss.item():.4f}, \"\n",
        "                      f\"Kappa: {kappa:.2f}, Epsilon: {epsilon_train:.2f}, \"\n",
        "                      f\"Standard Acc: {standard_acc:.2f}%, Verified Acc: {verified_acc:.2f}%\")\n",
        "\n",
        "                if verified_acc > best_verified_acc:\n",
        "                    best_verified_acc = verified_acc\n",
        "                    patience_counter = 0\n",
        "                    torch.save(model.state_dict(), 'best_model.pth')\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "                    if patience_counter >= patience:\n",
        "                        print(\"Early stopping triggered.\")\n",
        "                        return\n",
        "            current_step += 1\n",
        "\n",
        "    print(\"IBP Training completed.\")"
      ],
      "metadata": {
        "id": "iHxwkr3MaFH9"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_verified_accuracy(model, data_loader, eps, device='cpu', margin=1e-6):\n",
        "    model.eval()\n",
        "    correct_verified = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_data, batch_labels in data_loader:\n",
        "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "            lower_logits, upper_logits = ibp_forward(model, batch_data, eps)\n",
        "\n",
        "            if model.fc2.out_features == 2:\n",
        "                lower_true = torch.where(batch_labels == 1, lower_logits[:,1], lower_logits[:,0])\n",
        "                upper_other = torch.where(batch_labels == 1, upper_logits[:,0], upper_logits[:,1])\n",
        "                verified = (lower_true - upper_other) > margin\n",
        "                correct_verified += verified.sum().item()\n",
        "                total += batch_labels.size(0)\n",
        "            else:\n",
        "                raise NotImplementedError(\"Verified accuracy computation is only implemented for binary classification.\")\n",
        "\n",
        "    verified_accuracy = (correct_verified / total) * 100\n",
        "    return verified_accuracy\n"
      ],
      "metadata": {
        "id": "CtInwQabaMVx"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_standard_accuracy(model, data_loader, device='cpu'):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_data, batch_labels in data_loader:\n",
        "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "            logits = model(batch_data)\n",
        "            predictions = torch.argmax(logits, dim=1)\n",
        "            correct += (predictions == batch_labels).sum().item()\n",
        "            total += batch_labels.size(0)\n",
        "    accuracy = (correct / total) * 100\n",
        "    return accuracy\n",
        "\n",
        "def evaluate(model, train_loader, val_loader, test_loader, eps, device='cpu'):\n",
        "    standard_train_acc = compute_standard_accuracy(model, train_loader, device)\n",
        "    standard_val_acc = compute_standard_accuracy(model, val_loader, device)\n",
        "    standard_test_acc = compute_standard_accuracy(model, test_loader, device)\n",
        "\n",
        "    verified_train_acc = compute_verified_accuracy(model, train_loader, eps, device)\n",
        "    verified_val_acc = compute_verified_accuracy(model, val_loader, eps, device)\n",
        "    verified_test_acc = compute_verified_accuracy(model, test_loader, eps, device)\n",
        "\n",
        "    print(f\"Standard Training Accuracy: {standard_train_acc:.2f}%\")\n",
        "    print(f\"Verified Training Accuracy: {verified_train_acc:.2f}%\")\n",
        "    print(f\"Standard Validation Accuracy: {standard_val_acc:.2f}%\")\n",
        "    print(f\"Verified Validation Accuracy: {verified_val_acc:.2f}%\")\n",
        "    print(f\"Standard Test Accuracy: {standard_test_acc:.2f}%\")\n",
        "    print(f\"Verified Test Accuracy: {verified_test_acc:.2f}%\")\n"
      ],
      "metadata": {
        "id": "gF9zzw-ZaNXP"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "model = TextIBPModel(embedding_dim=100, hidden_dim=100, num_classes=2).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "total_steps = 10000\n",
        "eps_target = 0.1\n",
        "train_ibp_with_early_stopping(model, optimizer, train_loader, val_loader, total_steps, eps_target, device)\n",
        "\n",
        "print(\"\\nEvaluation Results:\")\n",
        "evaluate(model, train_loader, val_loader, test_loader, eps_target, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVFUwR3IaQ6l",
        "outputId": "22854925-6df8-4a3b-ce0b-c26ac6e2fef9"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Step [0/10000], Loss: 0.6940, Kappa: 1.00, Epsilon: 0.00, Standard Acc: 49.16%, Verified Acc: 48.60%\n",
            "Step [100/10000], Loss: 0.6331, Kappa: 0.99, Epsilon: 0.00, Standard Acc: 66.52%, Verified Acc: 62.96%\n",
            "Step [200/10000], Loss: 0.6590, Kappa: 0.99, Epsilon: 0.00, Standard Acc: 74.92%, Verified Acc: 72.84%\n",
            "Step [300/10000], Loss: 0.4967, Kappa: 0.98, Epsilon: 0.00, Standard Acc: 75.20%, Verified Acc: 74.08%\n",
            "Step [400/10000], Loss: 0.5082, Kappa: 0.98, Epsilon: 0.00, Standard Acc: 76.60%, Verified Acc: 75.40%\n",
            "Step [500/10000], Loss: 0.4553, Kappa: 0.97, Epsilon: 0.01, Standard Acc: 77.60%, Verified Acc: 75.88%\n",
            "Step [600/10000], Loss: 0.7679, Kappa: 0.97, Epsilon: 0.01, Standard Acc: 76.88%, Verified Acc: 74.64%\n",
            "Step [700/10000], Loss: 0.4601, Kappa: 0.96, Epsilon: 0.01, Standard Acc: 77.24%, Verified Acc: 76.12%\n",
            "Step [800/10000], Loss: 0.5525, Kappa: 0.96, Epsilon: 0.01, Standard Acc: 77.36%, Verified Acc: 76.28%\n",
            "Step [900/10000], Loss: 0.4512, Kappa: 0.95, Epsilon: 0.01, Standard Acc: 77.72%, Verified Acc: 75.56%\n",
            "Step [1000/10000], Loss: 0.5621, Kappa: 0.95, Epsilon: 0.01, Standard Acc: 78.56%, Verified Acc: 77.24%\n",
            "Step [1100/10000], Loss: 0.4177, Kappa: 0.94, Epsilon: 0.01, Standard Acc: 75.44%, Verified Acc: 73.68%\n",
            "Step [1200/10000], Loss: 0.3344, Kappa: 0.94, Epsilon: 0.01, Standard Acc: 78.12%, Verified Acc: 76.52%\n",
            "Step [1300/10000], Loss: 0.4433, Kappa: 0.94, Epsilon: 0.01, Standard Acc: 77.00%, Verified Acc: 75.72%\n",
            "Step [1400/10000], Loss: 0.5127, Kappa: 0.93, Epsilon: 0.01, Standard Acc: 78.80%, Verified Acc: 77.84%\n",
            "Step [1500/10000], Loss: 0.3446, Kappa: 0.93, Epsilon: 0.01, Standard Acc: 76.72%, Verified Acc: 76.00%\n",
            "Step [1600/10000], Loss: 0.4217, Kappa: 0.92, Epsilon: 0.02, Standard Acc: 79.56%, Verified Acc: 78.72%\n",
            "Step [1700/10000], Loss: 0.4395, Kappa: 0.92, Epsilon: 0.02, Standard Acc: 79.44%, Verified Acc: 78.40%\n",
            "Step [1800/10000], Loss: 0.3734, Kappa: 0.91, Epsilon: 0.02, Standard Acc: 79.04%, Verified Acc: 78.24%\n",
            "Step [1900/10000], Loss: 0.4381, Kappa: 0.91, Epsilon: 0.02, Standard Acc: 79.36%, Verified Acc: 78.72%\n",
            "Step [2000/10000], Loss: 0.6206, Kappa: 0.90, Epsilon: 0.02, Standard Acc: 79.60%, Verified Acc: 78.72%\n",
            "Step [2100/10000], Loss: 0.4792, Kappa: 0.90, Epsilon: 0.02, Standard Acc: 79.12%, Verified Acc: 78.24%\n",
            "Step [2200/10000], Loss: 0.3899, Kappa: 0.89, Epsilon: 0.02, Standard Acc: 79.00%, Verified Acc: 78.12%\n",
            "Step [2300/10000], Loss: 0.4203, Kappa: 0.89, Epsilon: 0.02, Standard Acc: 79.32%, Verified Acc: 78.48%\n",
            "Step [2400/10000], Loss: 0.4377, Kappa: 0.88, Epsilon: 0.02, Standard Acc: 79.32%, Verified Acc: 78.48%\n",
            "Step [2500/10000], Loss: 0.4231, Kappa: 0.88, Epsilon: 0.03, Standard Acc: 80.32%, Verified Acc: 79.76%\n",
            "Step [2600/10000], Loss: 0.4038, Kappa: 0.87, Epsilon: 0.03, Standard Acc: 79.88%, Verified Acc: 79.44%\n",
            "Step [2700/10000], Loss: 0.5849, Kappa: 0.86, Epsilon: 0.03, Standard Acc: 78.76%, Verified Acc: 78.20%\n",
            "Step [2800/10000], Loss: 0.4270, Kappa: 0.86, Epsilon: 0.03, Standard Acc: 80.72%, Verified Acc: 80.08%\n",
            "Step [2900/10000], Loss: 0.4376, Kappa: 0.85, Epsilon: 0.03, Standard Acc: 80.48%, Verified Acc: 79.64%\n",
            "Step [3000/10000], Loss: 0.5476, Kappa: 0.85, Epsilon: 0.03, Standard Acc: 79.16%, Verified Acc: 78.60%\n",
            "Step [3100/10000], Loss: 0.3548, Kappa: 0.84, Epsilon: 0.03, Standard Acc: 80.00%, Verified Acc: 79.48%\n",
            "Step [3200/10000], Loss: 0.3262, Kappa: 0.84, Epsilon: 0.03, Standard Acc: 80.12%, Verified Acc: 79.24%\n",
            "Step [3300/10000], Loss: 0.5452, Kappa: 0.83, Epsilon: 0.03, Standard Acc: 79.92%, Verified Acc: 79.36%\n",
            "Step [3400/10000], Loss: 0.3252, Kappa: 0.83, Epsilon: 0.03, Standard Acc: 79.64%, Verified Acc: 79.12%\n",
            "Step [3500/10000], Loss: 0.3732, Kappa: 0.82, Epsilon: 0.03, Standard Acc: 81.04%, Verified Acc: 80.12%\n",
            "Step [3600/10000], Loss: 0.5860, Kappa: 0.82, Epsilon: 0.04, Standard Acc: 80.00%, Verified Acc: 79.28%\n",
            "Step [3700/10000], Loss: 0.4175, Kappa: 0.81, Epsilon: 0.04, Standard Acc: 80.04%, Verified Acc: 79.16%\n",
            "Step [3800/10000], Loss: 0.4827, Kappa: 0.81, Epsilon: 0.04, Standard Acc: 79.16%, Verified Acc: 78.92%\n",
            "Step [3900/10000], Loss: 0.3642, Kappa: 0.80, Epsilon: 0.04, Standard Acc: 81.16%, Verified Acc: 80.56%\n",
            "Step [4000/10000], Loss: 0.2844, Kappa: 0.80, Epsilon: 0.04, Standard Acc: 80.24%, Verified Acc: 79.56%\n",
            "Step [4100/10000], Loss: 0.5388, Kappa: 0.80, Epsilon: 0.04, Standard Acc: 81.20%, Verified Acc: 80.32%\n",
            "Step [4200/10000], Loss: 0.4070, Kappa: 0.79, Epsilon: 0.04, Standard Acc: 79.88%, Verified Acc: 79.16%\n",
            "Step [4300/10000], Loss: 0.4149, Kappa: 0.79, Epsilon: 0.04, Standard Acc: 80.56%, Verified Acc: 80.00%\n",
            "Step [4400/10000], Loss: 0.3334, Kappa: 0.78, Epsilon: 0.04, Standard Acc: 78.12%, Verified Acc: 77.64%\n",
            "Step [4500/10000], Loss: 0.4341, Kappa: 0.78, Epsilon: 0.05, Standard Acc: 80.64%, Verified Acc: 79.76%\n",
            "Step [4600/10000], Loss: 0.5456, Kappa: 0.77, Epsilon: 0.05, Standard Acc: 80.72%, Verified Acc: 79.96%\n",
            "Step [4700/10000], Loss: 0.3802, Kappa: 0.77, Epsilon: 0.05, Standard Acc: 80.52%, Verified Acc: 79.80%\n",
            "Step [4800/10000], Loss: 0.4800, Kappa: 0.76, Epsilon: 0.05, Standard Acc: 81.08%, Verified Acc: 80.40%\n",
            "Step [4900/10000], Loss: 0.4048, Kappa: 0.76, Epsilon: 0.05, Standard Acc: 80.84%, Verified Acc: 80.24%\n",
            "Step [5000/10000], Loss: 0.5034, Kappa: 0.75, Epsilon: 0.05, Standard Acc: 81.04%, Verified Acc: 80.44%\n",
            "Step [5100/10000], Loss: 0.4745, Kappa: 0.74, Epsilon: 0.05, Standard Acc: 78.64%, Verified Acc: 77.96%\n",
            "Step [5200/10000], Loss: 0.4217, Kappa: 0.74, Epsilon: 0.05, Standard Acc: 80.68%, Verified Acc: 79.96%\n",
            "Step [5300/10000], Loss: 0.5603, Kappa: 0.73, Epsilon: 0.05, Standard Acc: 79.56%, Verified Acc: 79.24%\n",
            "Step [5400/10000], Loss: 0.4539, Kappa: 0.73, Epsilon: 0.05, Standard Acc: 80.60%, Verified Acc: 80.64%\n",
            "Step [5500/10000], Loss: 0.4524, Kappa: 0.72, Epsilon: 0.06, Standard Acc: 81.44%, Verified Acc: 80.80%\n",
            "Step [5600/10000], Loss: 0.3357, Kappa: 0.72, Epsilon: 0.06, Standard Acc: 80.60%, Verified Acc: 80.12%\n",
            "Step [5700/10000], Loss: 0.3957, Kappa: 0.72, Epsilon: 0.06, Standard Acc: 81.28%, Verified Acc: 80.84%\n",
            "Step [5800/10000], Loss: 0.5060, Kappa: 0.71, Epsilon: 0.06, Standard Acc: 81.36%, Verified Acc: 81.00%\n",
            "Step [5900/10000], Loss: 0.5003, Kappa: 0.71, Epsilon: 0.06, Standard Acc: 79.96%, Verified Acc: 79.68%\n",
            "Step [6000/10000], Loss: 0.4644, Kappa: 0.70, Epsilon: 0.06, Standard Acc: 81.32%, Verified Acc: 80.84%\n",
            "Step [6100/10000], Loss: 0.3719, Kappa: 0.70, Epsilon: 0.06, Standard Acc: 81.00%, Verified Acc: 80.88%\n",
            "Step [6200/10000], Loss: 0.4118, Kappa: 0.69, Epsilon: 0.06, Standard Acc: 81.04%, Verified Acc: 80.64%\n",
            "Step [6300/10000], Loss: 0.4760, Kappa: 0.69, Epsilon: 0.06, Standard Acc: 79.32%, Verified Acc: 78.80%\n",
            "Step [6400/10000], Loss: 0.5306, Kappa: 0.68, Epsilon: 0.06, Standard Acc: 81.92%, Verified Acc: 81.32%\n",
            "Step [6500/10000], Loss: 0.3671, Kappa: 0.68, Epsilon: 0.07, Standard Acc: 80.28%, Verified Acc: 80.08%\n",
            "Step [6600/10000], Loss: 0.2798, Kappa: 0.67, Epsilon: 0.07, Standard Acc: 81.04%, Verified Acc: 80.52%\n",
            "Step [6700/10000], Loss: 0.4287, Kappa: 0.67, Epsilon: 0.07, Standard Acc: 81.32%, Verified Acc: 80.40%\n",
            "Step [6800/10000], Loss: 0.4799, Kappa: 0.66, Epsilon: 0.07, Standard Acc: 81.36%, Verified Acc: 80.44%\n",
            "Step [6900/10000], Loss: 0.4716, Kappa: 0.66, Epsilon: 0.07, Standard Acc: 79.84%, Verified Acc: 79.24%\n",
            "Step [7000/10000], Loss: 0.4258, Kappa: 0.65, Epsilon: 0.07, Standard Acc: 80.72%, Verified Acc: 80.40%\n",
            "Step [7100/10000], Loss: 0.4221, Kappa: 0.65, Epsilon: 0.07, Standard Acc: 73.16%, Verified Acc: 72.52%\n",
            "Step [7200/10000], Loss: 0.4790, Kappa: 0.64, Epsilon: 0.07, Standard Acc: 81.40%, Verified Acc: 80.84%\n",
            "Step [7300/10000], Loss: 0.3824, Kappa: 0.64, Epsilon: 0.07, Standard Acc: 79.80%, Verified Acc: 79.44%\n",
            "Step [7400/10000], Loss: 0.4854, Kappa: 0.63, Epsilon: 0.07, Standard Acc: 79.72%, Verified Acc: 79.28%\n",
            "Step [7500/10000], Loss: 0.3757, Kappa: 0.62, Epsilon: 0.08, Standard Acc: 82.08%, Verified Acc: 81.40%\n",
            "Step [7600/10000], Loss: 0.4194, Kappa: 0.62, Epsilon: 0.08, Standard Acc: 81.48%, Verified Acc: 80.92%\n",
            "Step [7700/10000], Loss: 0.5340, Kappa: 0.61, Epsilon: 0.08, Standard Acc: 81.04%, Verified Acc: 80.68%\n",
            "Step [7800/10000], Loss: 0.3987, Kappa: 0.61, Epsilon: 0.08, Standard Acc: 80.88%, Verified Acc: 80.48%\n",
            "Step [7900/10000], Loss: 0.3410, Kappa: 0.60, Epsilon: 0.08, Standard Acc: 81.28%, Verified Acc: 81.12%\n",
            "Step [8000/10000], Loss: 0.3312, Kappa: 0.60, Epsilon: 0.08, Standard Acc: 80.64%, Verified Acc: 80.24%\n",
            "Step [8100/10000], Loss: 0.4245, Kappa: 0.59, Epsilon: 0.08, Standard Acc: 80.76%, Verified Acc: 80.36%\n",
            "Step [8200/10000], Loss: 0.4698, Kappa: 0.59, Epsilon: 0.08, Standard Acc: 80.56%, Verified Acc: 80.24%\n",
            "Step [8300/10000], Loss: 0.3836, Kappa: 0.58, Epsilon: 0.08, Standard Acc: 81.80%, Verified Acc: 81.60%\n",
            "Step [8400/10000], Loss: 0.3700, Kappa: 0.58, Epsilon: 0.08, Standard Acc: 81.40%, Verified Acc: 80.68%\n",
            "Step [8500/10000], Loss: 0.3951, Kappa: 0.57, Epsilon: 0.09, Standard Acc: 80.64%, Verified Acc: 80.48%\n",
            "Step [8600/10000], Loss: 0.4058, Kappa: 0.57, Epsilon: 0.09, Standard Acc: 80.44%, Verified Acc: 80.24%\n",
            "Step [8700/10000], Loss: 0.2934, Kappa: 0.56, Epsilon: 0.09, Standard Acc: 80.80%, Verified Acc: 80.36%\n",
            "Step [8800/10000], Loss: 0.4108, Kappa: 0.56, Epsilon: 0.09, Standard Acc: 80.76%, Verified Acc: 80.52%\n",
            "Step [8900/10000], Loss: 0.4424, Kappa: 0.55, Epsilon: 0.09, Standard Acc: 81.76%, Verified Acc: 81.20%\n",
            "Step [9000/10000], Loss: 0.4202, Kappa: 0.55, Epsilon: 0.09, Standard Acc: 81.16%, Verified Acc: 81.04%\n",
            "Step [9100/10000], Loss: 0.5636, Kappa: 0.54, Epsilon: 0.09, Standard Acc: 80.16%, Verified Acc: 80.08%\n",
            "Step [9200/10000], Loss: 0.4402, Kappa: 0.54, Epsilon: 0.09, Standard Acc: 79.92%, Verified Acc: 79.60%\n",
            "Step [9300/10000], Loss: 0.3548, Kappa: 0.53, Epsilon: 0.09, Standard Acc: 80.84%, Verified Acc: 80.48%\n",
            "Step [9400/10000], Loss: 0.3807, Kappa: 0.53, Epsilon: 0.09, Standard Acc: 81.72%, Verified Acc: 81.36%\n",
            "Step [9500/10000], Loss: 0.3588, Kappa: 0.53, Epsilon: 0.10, Standard Acc: 81.20%, Verified Acc: 81.12%\n",
            "Step [9600/10000], Loss: 0.3246, Kappa: 0.52, Epsilon: 0.10, Standard Acc: 80.44%, Verified Acc: 80.28%\n",
            "Step [9700/10000], Loss: 0.5059, Kappa: 0.52, Epsilon: 0.10, Standard Acc: 79.56%, Verified Acc: 79.28%\n",
            "Step [9800/10000], Loss: 0.4045, Kappa: 0.51, Epsilon: 0.10, Standard Acc: 81.60%, Verified Acc: 81.12%\n",
            "Step [9900/10000], Loss: 0.4385, Kappa: 0.51, Epsilon: 0.10, Standard Acc: 80.92%, Verified Acc: 80.76%\n",
            "IBP Training completed.\n",
            "\n",
            "Evaluation Results:\n",
            "Standard Training Accuracy: 81.17%\n",
            "Verified Training Accuracy: 80.80%\n",
            "Standard Validation Accuracy: 82.08%\n",
            "Verified Validation Accuracy: 81.56%\n",
            "Standard Test Accuracy: 80.33%\n",
            "Verified Test Accuracy: 80.10%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_standard(model, optimizer, train_loader, val_loader, total_steps, device='cpu'):\n",
        "    model.train()\n",
        "    current_step = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    while current_step < total_steps:\n",
        "        for batch_data, batch_labels in train_loader:\n",
        "            if current_step >= total_steps:\n",
        "                break\n",
        "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "\n",
        "            logits = model(batch_data)\n",
        "            loss = F.cross_entropy(logits, batch_labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if current_step % 100 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                print(f\"Step [{current_step}/{total_steps}], Loss: {loss.item():.4f}, Time Elapsed: {elapsed:.2f}s\")\n",
        "                start_time = time.time()\n",
        "\n",
        "            current_step += 1\n",
        "\n",
        "    print(\"Standard training completed.\")\n"
      ],
      "metadata": {
        "id": "FFVbKYJkaTR4"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "# im copying the model so its fresh\n",
        "\n",
        "standard_model = TextIBPModel(embedding_dim=100, hidden_dim=100, num_classes=2).to(device)\n",
        "standard_optimizer = optim.Adam(standard_model.parameters(), lr=1e-3)\n",
        "\n",
        "start_time = time.time()\n",
        "train_standard(standard_model, standard_optimizer, train_loader, val_loader, total_steps, device)\n",
        "standard_training_time = time.time() - start_time\n",
        "print(f\"Standard Training Time: {standard_training_time:.2f}s\")\n",
        "\n",
        "model_ibp = TextIBPModel(embedding_dim=100, hidden_dim=100, num_classes=2).to(device)\n",
        "optimizer_ibp = optim.Adam(model_ibp.parameters(), lr=1e-3)\n",
        "\n",
        "start_time = time.time()\n",
        "train_ibp(model_ibp, optimizer_ibp, train_loader, val_loader, total_steps, eps_target, device)\n",
        "ibp_training_time = time.time() - start_time\n",
        "print(f\"IBP Training Time: {ibp_training_time:.2f}s\")\n",
        "\n",
        "print(f\"IBP Training is {ibp_training_time / standard_training_time:.2f} times the standard training time.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgofKFoIaVEx",
        "outputId": "71a6d429-a11b-4d89-99e2-32aca0d2af16"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step [0/10000], Loss: 0.6888, Time Elapsed: 0.00s\n",
            "Step [100/10000], Loss: 0.5636, Time Elapsed: 0.18s\n",
            "Step [200/10000], Loss: 0.4850, Time Elapsed: 0.19s\n",
            "Step [300/10000], Loss: 0.4658, Time Elapsed: 0.19s\n",
            "Step [400/10000], Loss: 0.5137, Time Elapsed: 0.19s\n",
            "Step [500/10000], Loss: 0.4374, Time Elapsed: 0.18s\n",
            "Step [600/10000], Loss: 0.3360, Time Elapsed: 0.18s\n",
            "Step [700/10000], Loss: 0.5091, Time Elapsed: 0.18s\n",
            "Step [800/10000], Loss: 0.4713, Time Elapsed: 0.18s\n",
            "Step [900/10000], Loss: 0.3690, Time Elapsed: 0.18s\n",
            "Step [1000/10000], Loss: 0.4528, Time Elapsed: 0.19s\n",
            "Step [1100/10000], Loss: 0.3663, Time Elapsed: 0.20s\n",
            "Step [1200/10000], Loss: 0.5491, Time Elapsed: 0.19s\n",
            "Step [1300/10000], Loss: 0.4720, Time Elapsed: 0.19s\n",
            "Step [1400/10000], Loss: 0.5354, Time Elapsed: 0.19s\n",
            "Step [1500/10000], Loss: 0.3700, Time Elapsed: 0.19s\n",
            "Step [1600/10000], Loss: 0.5450, Time Elapsed: 0.19s\n",
            "Step [1700/10000], Loss: 0.4592, Time Elapsed: 0.20s\n",
            "Step [1800/10000], Loss: 0.3722, Time Elapsed: 0.19s\n",
            "Step [1900/10000], Loss: 0.4290, Time Elapsed: 0.18s\n",
            "Step [2000/10000], Loss: 0.7189, Time Elapsed: 0.18s\n",
            "Step [2100/10000], Loss: 0.5336, Time Elapsed: 0.18s\n",
            "Step [2200/10000], Loss: 0.4344, Time Elapsed: 0.18s\n",
            "Step [2300/10000], Loss: 0.3312, Time Elapsed: 0.18s\n",
            "Step [2400/10000], Loss: 0.3853, Time Elapsed: 0.18s\n",
            "Step [2500/10000], Loss: 0.5211, Time Elapsed: 0.18s\n",
            "Step [2600/10000], Loss: 0.4722, Time Elapsed: 0.18s\n",
            "Step [2700/10000], Loss: 0.4201, Time Elapsed: 0.18s\n",
            "Step [2800/10000], Loss: 0.6605, Time Elapsed: 0.18s\n",
            "Step [2900/10000], Loss: 0.3919, Time Elapsed: 0.18s\n",
            "Step [3000/10000], Loss: 0.4054, Time Elapsed: 0.18s\n",
            "Step [3100/10000], Loss: 0.5045, Time Elapsed: 0.18s\n",
            "Step [3200/10000], Loss: 0.3583, Time Elapsed: 0.18s\n",
            "Step [3300/10000], Loss: 0.4534, Time Elapsed: 0.18s\n",
            "Step [3400/10000], Loss: 0.3793, Time Elapsed: 0.18s\n",
            "Step [3500/10000], Loss: 0.3963, Time Elapsed: 0.18s\n",
            "Step [3600/10000], Loss: 0.3820, Time Elapsed: 0.18s\n",
            "Step [3700/10000], Loss: 0.4614, Time Elapsed: 0.18s\n",
            "Step [3800/10000], Loss: 0.4192, Time Elapsed: 0.18s\n",
            "Step [3900/10000], Loss: 0.3359, Time Elapsed: 0.18s\n",
            "Step [4000/10000], Loss: 0.3738, Time Elapsed: 0.18s\n",
            "Step [4100/10000], Loss: 0.3868, Time Elapsed: 0.18s\n",
            "Step [4200/10000], Loss: 0.3519, Time Elapsed: 0.18s\n",
            "Step [4300/10000], Loss: 0.4023, Time Elapsed: 0.19s\n",
            "Step [4400/10000], Loss: 0.4365, Time Elapsed: 0.18s\n",
            "Step [4500/10000], Loss: 0.3934, Time Elapsed: 0.18s\n",
            "Step [4600/10000], Loss: 0.3934, Time Elapsed: 0.18s\n",
            "Step [4700/10000], Loss: 0.3991, Time Elapsed: 0.18s\n",
            "Step [4800/10000], Loss: 0.5316, Time Elapsed: 0.18s\n",
            "Step [4900/10000], Loss: 0.5513, Time Elapsed: 0.18s\n",
            "Step [5000/10000], Loss: 0.4607, Time Elapsed: 0.18s\n",
            "Step [5100/10000], Loss: 0.4146, Time Elapsed: 0.18s\n",
            "Step [5200/10000], Loss: 0.4462, Time Elapsed: 0.18s\n",
            "Step [5300/10000], Loss: 0.5399, Time Elapsed: 0.18s\n",
            "Step [5400/10000], Loss: 0.3237, Time Elapsed: 0.18s\n",
            "Step [5500/10000], Loss: 0.4289, Time Elapsed: 0.18s\n",
            "Step [5600/10000], Loss: 0.4250, Time Elapsed: 0.18s\n",
            "Step [5700/10000], Loss: 0.2964, Time Elapsed: 0.18s\n",
            "Step [5800/10000], Loss: 0.5081, Time Elapsed: 0.18s\n",
            "Step [5900/10000], Loss: 0.3376, Time Elapsed: 0.18s\n",
            "Step [6000/10000], Loss: 0.3705, Time Elapsed: 0.18s\n",
            "Step [6100/10000], Loss: 0.4340, Time Elapsed: 0.18s\n",
            "Step [6200/10000], Loss: 0.5225, Time Elapsed: 0.18s\n",
            "Step [6300/10000], Loss: 0.4317, Time Elapsed: 0.18s\n",
            "Step [6400/10000], Loss: 0.4515, Time Elapsed: 0.18s\n",
            "Step [6500/10000], Loss: 0.3919, Time Elapsed: 0.18s\n",
            "Step [6600/10000], Loss: 0.5086, Time Elapsed: 0.18s\n",
            "Step [6700/10000], Loss: 0.4306, Time Elapsed: 0.18s\n",
            "Step [6800/10000], Loss: 0.3842, Time Elapsed: 0.18s\n",
            "Step [6900/10000], Loss: 0.3761, Time Elapsed: 0.18s\n",
            "Step [7000/10000], Loss: 0.4338, Time Elapsed: 0.18s\n",
            "Step [7100/10000], Loss: 0.3378, Time Elapsed: 0.18s\n",
            "Step [7200/10000], Loss: 0.4346, Time Elapsed: 0.18s\n",
            "Step [7300/10000], Loss: 0.4351, Time Elapsed: 0.19s\n",
            "Step [7400/10000], Loss: 0.4235, Time Elapsed: 0.19s\n",
            "Step [7500/10000], Loss: 0.4967, Time Elapsed: 0.19s\n",
            "Step [7600/10000], Loss: 0.4233, Time Elapsed: 0.18s\n",
            "Step [7700/10000], Loss: 0.4428, Time Elapsed: 0.19s\n",
            "Step [7800/10000], Loss: 0.4209, Time Elapsed: 0.19s\n",
            "Step [7900/10000], Loss: 0.3636, Time Elapsed: 0.19s\n",
            "Step [8000/10000], Loss: 0.4790, Time Elapsed: 0.19s\n",
            "Step [8100/10000], Loss: 0.3878, Time Elapsed: 0.20s\n",
            "Step [8200/10000], Loss: 0.4564, Time Elapsed: 0.18s\n",
            "Step [8300/10000], Loss: 0.4090, Time Elapsed: 0.18s\n",
            "Step [8400/10000], Loss: 0.3868, Time Elapsed: 0.18s\n",
            "Step [8500/10000], Loss: 0.4949, Time Elapsed: 0.18s\n",
            "Step [8600/10000], Loss: 0.4022, Time Elapsed: 0.18s\n",
            "Step [8700/10000], Loss: 0.4224, Time Elapsed: 0.18s\n",
            "Step [8800/10000], Loss: 0.4390, Time Elapsed: 0.18s\n",
            "Step [8900/10000], Loss: 0.2960, Time Elapsed: 0.18s\n",
            "Step [9000/10000], Loss: 0.3733, Time Elapsed: 0.18s\n",
            "Step [9100/10000], Loss: 0.3697, Time Elapsed: 0.18s\n",
            "Step [9200/10000], Loss: 0.4637, Time Elapsed: 0.18s\n",
            "Step [9300/10000], Loss: 0.3661, Time Elapsed: 0.18s\n",
            "Step [9400/10000], Loss: 0.3658, Time Elapsed: 0.18s\n",
            "Step [9500/10000], Loss: 0.4241, Time Elapsed: 0.18s\n",
            "Step [9600/10000], Loss: 0.4706, Time Elapsed: 0.18s\n",
            "Step [9700/10000], Loss: 0.4516, Time Elapsed: 0.18s\n",
            "Step [9800/10000], Loss: 0.3824, Time Elapsed: 0.18s\n",
            "Step [9900/10000], Loss: 0.3820, Time Elapsed: 0.18s\n",
            "Standard training completed.\n",
            "Standard Training Time: 18.33s\n",
            "Step [0/10000], Loss: 0.6785, kappa: 1.00, epsilon_train: 0.00, Time Elapsed: 0.00s\n",
            "Step [100/10000], Loss: 0.7220, kappa: 0.99, epsilon_train: 0.00, Time Elapsed: 0.28s\n",
            "Step [200/10000], Loss: 0.4534, kappa: 0.99, epsilon_train: 0.00, Time Elapsed: 0.28s\n",
            "Step [300/10000], Loss: 0.4349, kappa: 0.98, epsilon_train: 0.00, Time Elapsed: 0.28s\n",
            "Step [400/10000], Loss: 0.5053, kappa: 0.98, epsilon_train: 0.00, Time Elapsed: 0.29s\n",
            "Step [500/10000], Loss: 0.5224, kappa: 0.97, epsilon_train: 0.01, Time Elapsed: 0.28s\n",
            "Step [600/10000], Loss: 0.3916, kappa: 0.97, epsilon_train: 0.01, Time Elapsed: 0.30s\n",
            "Step [700/10000], Loss: 0.4456, kappa: 0.96, epsilon_train: 0.01, Time Elapsed: 0.30s\n",
            "Step [800/10000], Loss: 0.4177, kappa: 0.96, epsilon_train: 0.01, Time Elapsed: 0.28s\n",
            "Step [900/10000], Loss: 0.4752, kappa: 0.95, epsilon_train: 0.01, Time Elapsed: 0.29s\n",
            "Step [1000/10000], Loss: 0.5516, kappa: 0.95, epsilon_train: 0.01, Time Elapsed: 0.33s\n",
            "Step [1100/10000], Loss: 0.3967, kappa: 0.94, epsilon_train: 0.01, Time Elapsed: 0.30s\n",
            "Step [1200/10000], Loss: 0.4621, kappa: 0.94, epsilon_train: 0.01, Time Elapsed: 0.28s\n",
            "Step [1300/10000], Loss: 0.4695, kappa: 0.94, epsilon_train: 0.01, Time Elapsed: 0.29s\n",
            "Step [1400/10000], Loss: 0.5743, kappa: 0.93, epsilon_train: 0.01, Time Elapsed: 0.29s\n",
            "Step [1500/10000], Loss: 0.3963, kappa: 0.93, epsilon_train: 0.01, Time Elapsed: 0.30s\n",
            "Step [1600/10000], Loss: 0.3036, kappa: 0.92, epsilon_train: 0.02, Time Elapsed: 0.28s\n",
            "Step [1700/10000], Loss: 0.3882, kappa: 0.92, epsilon_train: 0.02, Time Elapsed: 0.28s\n",
            "Step [1800/10000], Loss: 0.3612, kappa: 0.91, epsilon_train: 0.02, Time Elapsed: 0.28s\n",
            "Step [1900/10000], Loss: 0.4793, kappa: 0.91, epsilon_train: 0.02, Time Elapsed: 0.29s\n",
            "Step [2000/10000], Loss: 0.4893, kappa: 0.90, epsilon_train: 0.02, Time Elapsed: 0.29s\n",
            "Step [2100/10000], Loss: 0.4671, kappa: 0.90, epsilon_train: 0.02, Time Elapsed: 0.28s\n",
            "Step [2200/10000], Loss: 0.4336, kappa: 0.89, epsilon_train: 0.02, Time Elapsed: 0.29s\n",
            "Step [2300/10000], Loss: 0.5120, kappa: 0.89, epsilon_train: 0.02, Time Elapsed: 0.30s\n",
            "Step [2400/10000], Loss: 0.5586, kappa: 0.88, epsilon_train: 0.02, Time Elapsed: 0.29s\n",
            "Step [2500/10000], Loss: 0.3739, kappa: 0.88, epsilon_train: 0.03, Time Elapsed: 0.29s\n",
            "Step [2600/10000], Loss: 0.4014, kappa: 0.87, epsilon_train: 0.03, Time Elapsed: 0.30s\n",
            "Step [2700/10000], Loss: 0.6675, kappa: 0.86, epsilon_train: 0.03, Time Elapsed: 0.33s\n",
            "Step [2800/10000], Loss: 0.4408, kappa: 0.86, epsilon_train: 0.03, Time Elapsed: 0.33s\n",
            "Step [2900/10000], Loss: 0.3512, kappa: 0.85, epsilon_train: 0.03, Time Elapsed: 0.30s\n",
            "Step [3000/10000], Loss: 0.4124, kappa: 0.85, epsilon_train: 0.03, Time Elapsed: 0.31s\n",
            "Step [3100/10000], Loss: 0.4642, kappa: 0.84, epsilon_train: 0.03, Time Elapsed: 0.29s\n",
            "Step [3200/10000], Loss: 0.5235, kappa: 0.84, epsilon_train: 0.03, Time Elapsed: 0.30s\n",
            "Step [3300/10000], Loss: 0.5186, kappa: 0.83, epsilon_train: 0.03, Time Elapsed: 0.29s\n",
            "Step [3400/10000], Loss: 0.5010, kappa: 0.83, epsilon_train: 0.03, Time Elapsed: 0.29s\n",
            "Step [3500/10000], Loss: 0.4685, kappa: 0.82, epsilon_train: 0.03, Time Elapsed: 0.28s\n",
            "Step [3600/10000], Loss: 0.4587, kappa: 0.82, epsilon_train: 0.04, Time Elapsed: 0.30s\n",
            "Step [3700/10000], Loss: 0.4130, kappa: 0.81, epsilon_train: 0.04, Time Elapsed: 0.29s\n",
            "Step [3800/10000], Loss: 0.4233, kappa: 0.81, epsilon_train: 0.04, Time Elapsed: 0.29s\n",
            "Step [3900/10000], Loss: 0.3790, kappa: 0.80, epsilon_train: 0.04, Time Elapsed: 0.29s\n",
            "Step [4000/10000], Loss: 0.5499, kappa: 0.80, epsilon_train: 0.04, Time Elapsed: 0.29s\n",
            "Step [4100/10000], Loss: 0.4287, kappa: 0.80, epsilon_train: 0.04, Time Elapsed: 0.29s\n",
            "Step [4200/10000], Loss: 0.4781, kappa: 0.79, epsilon_train: 0.04, Time Elapsed: 0.29s\n",
            "Step [4300/10000], Loss: 0.4332, kappa: 0.79, epsilon_train: 0.04, Time Elapsed: 0.29s\n",
            "Step [4400/10000], Loss: 0.5187, kappa: 0.78, epsilon_train: 0.04, Time Elapsed: 0.29s\n",
            "Step [4500/10000], Loss: 0.4370, kappa: 0.78, epsilon_train: 0.05, Time Elapsed: 0.29s\n",
            "Step [4600/10000], Loss: 0.4727, kappa: 0.77, epsilon_train: 0.05, Time Elapsed: 0.29s\n",
            "Step [4700/10000], Loss: 0.4923, kappa: 0.77, epsilon_train: 0.05, Time Elapsed: 0.29s\n",
            "Step [4800/10000], Loss: 0.5348, kappa: 0.76, epsilon_train: 0.05, Time Elapsed: 0.28s\n",
            "Step [4900/10000], Loss: 0.4566, kappa: 0.76, epsilon_train: 0.05, Time Elapsed: 0.29s\n",
            "Step [5000/10000], Loss: 0.3697, kappa: 0.75, epsilon_train: 0.05, Time Elapsed: 0.29s\n",
            "Step [5100/10000], Loss: 0.4702, kappa: 0.74, epsilon_train: 0.05, Time Elapsed: 0.29s\n",
            "Step [5200/10000], Loss: 0.4356, kappa: 0.74, epsilon_train: 0.05, Time Elapsed: 0.28s\n",
            "Step [5300/10000], Loss: 0.3841, kappa: 0.73, epsilon_train: 0.05, Time Elapsed: 0.29s\n",
            "Step [5400/10000], Loss: 0.3831, kappa: 0.73, epsilon_train: 0.05, Time Elapsed: 0.29s\n",
            "Step [5500/10000], Loss: 0.5314, kappa: 0.72, epsilon_train: 0.06, Time Elapsed: 0.29s\n",
            "Step [5600/10000], Loss: 0.5068, kappa: 0.72, epsilon_train: 0.06, Time Elapsed: 0.28s\n",
            "Step [5700/10000], Loss: 0.4277, kappa: 0.72, epsilon_train: 0.06, Time Elapsed: 0.29s\n",
            "Step [5800/10000], Loss: 0.4874, kappa: 0.71, epsilon_train: 0.06, Time Elapsed: 0.29s\n",
            "Step [5900/10000], Loss: 0.3827, kappa: 0.71, epsilon_train: 0.06, Time Elapsed: 0.29s\n",
            "Step [6000/10000], Loss: 0.4388, kappa: 0.70, epsilon_train: 0.06, Time Elapsed: 0.29s\n",
            "Step [6100/10000], Loss: 0.5133, kappa: 0.70, epsilon_train: 0.06, Time Elapsed: 0.28s\n",
            "Step [6200/10000], Loss: 0.2953, kappa: 0.69, epsilon_train: 0.06, Time Elapsed: 0.29s\n",
            "Step [6300/10000], Loss: 0.4673, kappa: 0.69, epsilon_train: 0.06, Time Elapsed: 0.30s\n",
            "Step [6400/10000], Loss: 0.3931, kappa: 0.68, epsilon_train: 0.06, Time Elapsed: 0.30s\n",
            "Step [6500/10000], Loss: 0.3876, kappa: 0.68, epsilon_train: 0.07, Time Elapsed: 0.30s\n",
            "Step [6600/10000], Loss: 0.4802, kappa: 0.67, epsilon_train: 0.07, Time Elapsed: 0.30s\n",
            "Step [6700/10000], Loss: 0.3553, kappa: 0.67, epsilon_train: 0.07, Time Elapsed: 0.29s\n",
            "Step [6800/10000], Loss: 0.4399, kappa: 0.66, epsilon_train: 0.07, Time Elapsed: 0.30s\n",
            "Step [6900/10000], Loss: 0.3748, kappa: 0.66, epsilon_train: 0.07, Time Elapsed: 0.28s\n",
            "Step [7000/10000], Loss: 0.2930, kappa: 0.65, epsilon_train: 0.07, Time Elapsed: 0.29s\n",
            "Step [7100/10000], Loss: 0.4555, kappa: 0.65, epsilon_train: 0.07, Time Elapsed: 0.29s\n",
            "Step [7200/10000], Loss: 0.4131, kappa: 0.64, epsilon_train: 0.07, Time Elapsed: 0.29s\n",
            "Step [7300/10000], Loss: 0.4310, kappa: 0.64, epsilon_train: 0.07, Time Elapsed: 0.29s\n",
            "Step [7400/10000], Loss: 0.3506, kappa: 0.63, epsilon_train: 0.07, Time Elapsed: 0.29s\n",
            "Step [7500/10000], Loss: 0.4798, kappa: 0.62, epsilon_train: 0.08, Time Elapsed: 0.29s\n",
            "Step [7600/10000], Loss: 0.3429, kappa: 0.62, epsilon_train: 0.08, Time Elapsed: 0.29s\n",
            "Step [7700/10000], Loss: 0.5008, kappa: 0.61, epsilon_train: 0.08, Time Elapsed: 0.29s\n",
            "Step [7800/10000], Loss: 0.3766, kappa: 0.61, epsilon_train: 0.08, Time Elapsed: 0.29s\n",
            "Step [7900/10000], Loss: 0.3417, kappa: 0.60, epsilon_train: 0.08, Time Elapsed: 0.28s\n",
            "Step [8000/10000], Loss: 0.3712, kappa: 0.60, epsilon_train: 0.08, Time Elapsed: 0.28s\n",
            "Step [8100/10000], Loss: 0.4438, kappa: 0.59, epsilon_train: 0.08, Time Elapsed: 0.29s\n",
            "Step [8200/10000], Loss: 0.5200, kappa: 0.59, epsilon_train: 0.08, Time Elapsed: 0.29s\n",
            "Step [8300/10000], Loss: 0.3570, kappa: 0.58, epsilon_train: 0.08, Time Elapsed: 0.29s\n",
            "Step [8400/10000], Loss: 0.4074, kappa: 0.58, epsilon_train: 0.08, Time Elapsed: 0.30s\n",
            "Step [8500/10000], Loss: 0.5767, kappa: 0.57, epsilon_train: 0.09, Time Elapsed: 0.30s\n",
            "Step [8600/10000], Loss: 0.3206, kappa: 0.57, epsilon_train: 0.09, Time Elapsed: 0.30s\n",
            "Step [8700/10000], Loss: 0.4620, kappa: 0.56, epsilon_train: 0.09, Time Elapsed: 0.30s\n",
            "Step [8800/10000], Loss: 0.6066, kappa: 0.56, epsilon_train: 0.09, Time Elapsed: 0.30s\n",
            "Step [8900/10000], Loss: 0.3784, kappa: 0.55, epsilon_train: 0.09, Time Elapsed: 0.29s\n",
            "Step [9000/10000], Loss: 0.4917, kappa: 0.55, epsilon_train: 0.09, Time Elapsed: 0.29s\n",
            "Step [9100/10000], Loss: 0.4575, kappa: 0.54, epsilon_train: 0.09, Time Elapsed: 0.30s\n",
            "Step [9200/10000], Loss: 0.4149, kappa: 0.54, epsilon_train: 0.09, Time Elapsed: 0.29s\n",
            "Step [9300/10000], Loss: 0.5121, kappa: 0.53, epsilon_train: 0.09, Time Elapsed: 0.30s\n",
            "Step [9400/10000], Loss: 0.4227, kappa: 0.53, epsilon_train: 0.09, Time Elapsed: 0.29s\n",
            "Step [9500/10000], Loss: 0.3312, kappa: 0.53, epsilon_train: 0.10, Time Elapsed: 0.29s\n",
            "Step [9600/10000], Loss: 0.3797, kappa: 0.52, epsilon_train: 0.10, Time Elapsed: 0.29s\n",
            "Step [9700/10000], Loss: 0.4350, kappa: 0.52, epsilon_train: 0.10, Time Elapsed: 0.29s\n",
            "Step [9800/10000], Loss: 0.5677, kappa: 0.51, epsilon_train: 0.10, Time Elapsed: 0.29s\n",
            "Step [9900/10000], Loss: 0.3986, kappa: 0.51, epsilon_train: 0.10, Time Elapsed: 0.29s\n",
            "Training completed.\n",
            "IBP Training Time: 29.15s\n",
            "IBP Training is 1.59 times the standard training time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IyeROy-UaWb_"
      },
      "execution_count": 71,
      "outputs": []
    }
  ]
}